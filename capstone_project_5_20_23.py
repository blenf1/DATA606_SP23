# -*- coding: utf-8 -*-
"""Capstone Project 5-20-23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/183S-GC984KDbVpqUtJXjrfuuGMcs3B2o
"""

# Commented out IPython magic to ensure Python compatibility.
 #!hostname -I
from google.colab import drive
drive.mount('/content/drive')
#
!pwd
!ls
# %cd /content/drive/MyDrive/'Colab Notebooks'/DATA606Spring2023/Capstone
!pwd
!ls
import os
os.chdir("/content/drive/My Drive")
!ls

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_excel('Capstone-Crime_Data.xlsx')
df.head()

len(df)

df.info()

"""Data Preprocessing"""

#Converting current datetime to normal datetime
df['CrimeDateTime'] = pd.to_datetime(df['CrimeDateTime']).dt.date

df["Year"] = pd.DatetimeIndex(df['CrimeDateTime']).year
df['Month'] = pd.DatetimeIndex(df['CrimeDateTime']).month
df['Day'] = pd.DatetimeIndex(df['CrimeDateTime']).day
df['Hour'] = pd.DatetimeIndex(df['CrimeDateTime']).hour
df['Minute'] = pd.DatetimeIndex(df['CrimeDateTime']).minute
df.head(10)

print(df.isnull().sum())

df = df.drop(["Age", "Gender", "Race", "Ethnicity", "Weapon", "CCNO", "Inside_Outside", "Premise", "Total_Incidents"], axis = 1)

"""Exploratory Analysis"""

df['Description'].unique()

df['Description'].value_counts()

"""Data Visualization"""

plt.figure(figsize=(10,6))
df['Description'].value_counts().plot(kind='bar', color='#900C3F',title ='Number of Unqiue Crimes in Baltimore')

#Based on the live graph below, it appears the number of crimes have plumented from mid 2018 to 2022. This could be due to the mass shutdown during COVID
plt.figure(figsize=(10,8))
df.groupby('Year').size().plot(kind = 'line', color = '#0C5790', title='Total Crimes Per Year', fontsize=12)

plt.figure(figsize=(10,8))
df.groupby(('Month')).size().plot(kind ='bar', color = '#65A684', title = 'Total Crimes Per Month', xlabel='Months', ylabel='Number of Crimes')

plt.figure(figsize=(20,10))
#df.groupby(('Month','Description')).size().plot(kind ='bar', color = '#E6E271', title = 'Crime Rates by Month Based on Type of Crime')

crime_counts = df.groupby(['Month', 'Description'])['CrimeDateTime'].count()

# Pivot table to reshape data for plotting
crime_counts = crime_counts.reset_index().pivot(index='Month', columns='Description', values='CrimeDateTime')

# Plot line chart
crime_counts.plot()
plt.title('Crime Rates by Month')
plt.xlabel('Month')
plt.ylabel('Number of Crimes')
plt.legend(title='Crime Type',loc='center left', bbox_to_anchor=(1,0.5), fontsize=8, markerscale=0.5)
plt.show()

df.isna().sum()

df_geo = df.dropna(subset=['Latitude', 'Longitude'])

df_geo.isna().sum()

import folium
from folium.plugins import HeatMap

# Filter the data to include only the necessary columns for the heat map
heatmap_data = df_geo[["Latitude", "Longitude", "Description"]]

# Create a list to store the heat map data for each crime type
heat_data_list = []

# Create a list to store the crime types
crime_types = []

# Iterate over each crime type
for description in heatmap_data["Description"].unique():
    filtered_data = heatmap_data[heatmap_data["Description"] == description]
    heat_data = filtered_data[["Latitude", "Longitude"]].values
    heat_data_list.append(heat_data)
    crime_types.append(description)

# Create a folium map centered around Baltimore
baltimore_map = folium.Map(location=[39.2904, -76.6122], zoom_start=12)

# Create a HeatMap layer for each crime type
for heat_data, crime_type in zip(heat_data_list, crime_types):
    HeatMap(heat_data, name=crime_type).add_to(baltimore_map)

# Add layer control to the map
folium.LayerControl().add_to(baltimore_map)

# Display the map
baltimore_map

unique_crime_types = heatmap_data["Description"].unique()
print(unique_crime_types)

"""Four Specific Crimes"""

#Create a dataframe based on four frequent different crimes
description_filtered = df_geo['Description'].isin(['LARCENY', 'COMMON ASSAULT', 'BURGLARY', 'AGG. ASSAULT']) 
df_new = df_geo[description_filtered]

df_new.isna().sum()

df_new = df_new.dropna()

plt.figure(figsize=(10,6))
df_new.groupby('Description').size().plot(kind = 'bar', color = '#9FC185', title='Total Crimes Based on Description')

plt.figure(figsize=(20,10))
#df.groupby(('Month','Description')).size().plot(kind ='bar', color = '#E6E271', title = 'Crime Rates by Month Based on Type of Crime')

crime_counts = df_new.groupby(['Month', 'Description'])['CrimeDateTime'].count()

# Pivot table to reshape data for plotting
crime_counts = crime_counts.reset_index().pivot(index='Month', columns='Description', values='CrimeDateTime')

# Define a color map
colors = plt.get_cmap('Dark2')

# Plot line chart
crime_counts.plot(colormap=colors)
plt.title('Crime Rates by Month')
plt.xlabel('Month')
plt.ylabel('Number of Crimes')
plt.legend(title='Crime Type',loc='center left', bbox_to_anchor=(1,0.5), fontsize=8, markerscale=0.5)
plt.show()

plt.figure(figsize=(20,10))

ycrime_counts = df_new.groupby(['Year', 'Description'])['CrimeDateTime'].count()

# Pivot table to reshape data for plotting
ycrime_data = ycrime_counts.reset_index().pivot(index='Year', columns='Description', values='CrimeDateTime')

# Plot line chart
ycrime_data.plot(colormap=colors)
plt.title('Crime Rates by Year')
plt.xlabel('Year')
plt.ylabel('Number of Crimes')
plt.legend(title='Crime Type',loc='center left', bbox_to_anchor=(1,0.5), fontsize=8, markerscale=0.5)
plt.show()

import matplotlib.pyplot as plt

# Group the data by 'District' and 'Description' and count the occurrences
crime_counts = df_new.groupby(['District', 'Description']).size().unstack()

# Plot the bar chart
crime_counts.plot(kind='bar', stacked=True, figsize=(12, 6))
plt.title('Crime Counts by District and Description')
plt.xlabel('District')
plt.ylabel('Number of Crimes')
plt.legend(title='Description', fontsize='small')
plt.xticks(rotation=45)
plt.show()

Larceny = df_new[df_new.Description.str.contains("LARCENY")]
Common_Assault = df_new[df_new.Description.str.contains("COMMON ASSAULT")]
Burglary = df_new[df_new.Description.str.contains("BURGLARY")]
Agg_Assault = df_new[df_new.Description.str.contains("AGG. ASSAULT")]

Larceny.groupby("District").size().plot(kind = "barh", color="#4D1A47")
plt.title("Larceny")

Common_Assault.groupby("District").size().plot(kind = "barh", color="#1A3D4D")
plt.title("Common Assault")

Agg_Assault.groupby("District").size().plot(kind = "barh", color="#A95353")
plt.title("AGG. ASSAULT")

Burglary.groupby("District").size().plot(kind = "barh", color="#53A99A")
plt.title("BURGLARY")

"""Data Standardization


"""

from sklearn.preprocessing import MinMaxScaler
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

df_new.isna().sum()

df_new.info()

from sklearn.preprocessing import OneHotEncoder
le= preprocessing.LabelEncoder()
# Create a copy of the DataFrame
df_new_copy = df_new.copy()
df_new_copy["Neighborhood"] = le.fit_transform(df_new_copy["Neighborhood"])
df_new_copy["District"] = le.fit_transform(df_new_copy["District"])
df_new_copy["CrimeCode"] = le.fit_transform(df_new_copy["CrimeCode"])
df_new_copy["Description"] = le.fit_transform(df_new_copy["Description"])
df_new_copy.head(5)

# Select columns for correlation matrix
cols = ['Description', 'District', 'Neighborhood', 'CrimeCode','Month', 'Year']

# Calculate correlation matrix
corr_matrix = df_new_copy[cols].corr()

# Display the correlation matrix using a heatmap
sns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=True)
plt.show

df_new_copy.info()

print("Shape of input data:", df_new_copy.shape)

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


# select the features and target variable
features = ['District', 'Neighborhood', 'CrimeCode','Month', 'Year']
target = ['Description']

# split the data into training and testing sets
train, test = train_test_split(df_new_copy, test_size=0.3, random_state=42)


# create a random forest classifier with 100 trees
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# train the model on the training data
rf.fit(train[features], train[target].values.ravel())

# make predictions on the testing data
predictions = rf.predict(test[features])

# evaluate the accuracy of the model
accuracy = accuracy_score(test[target], predictions)
print('Accuracy:', accuracy)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# select the features and target variable
features = df_new_copy[['District', 'Neighborhood','CrimeCode', 'Month', 'Year']]
target = df_new_copy['Description']

clf = RandomForestClassifier()

# Specify the number of folds (e.g., 5-fold cross-validation)
num_folds = 5

# Perform cross-validation
clf_scores = cross_val_score(clf, features, target, cv=num_folds)

# Print the accuracy scores for each fold
print("Accuracy scores:", clf_scores)

# Calculate and print the mean accuracy score
mean_accuracy = clf_scores.mean()
print("Mean accuracy:", mean_accuracy)

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import classification_report

# Assuming you have defined your features (X) and target variable (y)

clf = RandomForestClassifier()

# Perform cross-validation predictions
y_pred = cross_val_predict(clf, features, target, cv=5)

# Generate classification report
report = classification_report(target, y_pred)

print(report)

plt.plot(range(1, 6), clf_scores, marker='o')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.title('Cross-Validation Results for Random Forest')
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, train_test_split

# Select the features and target variable
features = df_new_copy[['District', 'Neighborhood', 'CrimeCode', 'Month', 'Year']]
target = df_new_copy['Description']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)

# Create the RandomForestClassifier
clf_split = RandomForestClassifier()

# Fit the model on the training data
clf_split.fit(X_train, y_train)

# Perform cross-validation on the training data
num_folds = 5
clf1_scores = cross_val_score(clf_split, X_train, y_train, cv=num_folds)

# Print the accuracy scores for each fold
print("Accuracy scores:", clf1_scores)

# Calculate and print the mean accuracy score
mean_accuracy = clf1_scores.mean()
print("Mean accuracy:", mean_accuracy)

# Evaluate the model on the testing data
test_accuracy = clf_split.score(X_test, y_test)
print("Testing accuracy:", test_accuracy)

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import classification_report

# Assuming you have defined your features (X) and target variable (y)

clf_split = RandomForestClassifier()

# Perform cross-validation predictions
y_pred = cross_val_predict(clf_split, features, target, cv=5)

# Generate classification report
report = classification_report(target, y_pred)

print(report)

"""XGBoost Decision Tree Model """

import xgboost as xgb
from sklearn.metrics import accuracy_score
# select the features and target variable
features = ['District', 'Neighborhood','CrimeCode', 'Month', 'Year']
target = ['Description']

# split the data into training and testing sets
train, test = train_test_split(df_new_copy, test_size=0.3, random_state=42)
# Create XGBoost model
xgb_model = xgb.XGBClassifier()

# Train the model on the training data
xgb_model.fit(train[features], train[target].values.ravel())

# Make predictions on the test data
y_pred = xgb_model.predict(test[features])

# Calculate the accuracy of the model
accuracy = accuracy_score(test[target], y_pred)
print('Accuracy:', accuracy)

import xgboost as xgb
from sklearn.model_selection import cross_val_score

# Select the features and target variable
features = df_new_copy[['District', 'Neighborhood', 'CrimeCode', 'Month', 'Year']]
target = df_new_copy['Description']

# Create the XGBoost classifier object
xgb_classifier = xgb.XGBClassifier()

# Perform cross-validation
xgb_scores = cross_val_score(xgb_classifier, features, target, cv=5)  # Specify the number of folds (cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", xgb_scores)
print("Mean Accuracy:", xgb_scores.mean())

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import classification_report

# Assuming you have defined your features (X) and target variable (y)

xgb_classifier = xgb.XGBClassifier()

# Perform cross-validation predictions
xgb_y_pred = cross_val_predict(xgb_classifier, features, target, cv=5)

# Generate classification report
xgb_report = classification_report(target, xgb_y_pred)

print(xgb_report)

plt.plot(range(1, 6), xgb_scores, marker='o')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.title('Cross-Validation Results for XGBoost Model')
plt.show()

import xgboost as xgb
from sklearn.model_selection import cross_val_score, train_test_split

# Select the features and target variable
features = df_new_copy[['District', 'Neighborhood','CrimeCode', 'Month', 'Year']]
target = df_new_copy['Description']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)

# Create the XGBoost classifier object
xgb1_classifier = xgb.XGBClassifier()

# Perform cross-validation on the training data
num_folds = 5
xgb1_scores = cross_val_score(xgb1_classifier, X_train, y_train, cv=num_folds)

# Print the cross-validation scores
print("Cross-Validation Scores:", xgb1_scores)
print("Mean Accuracy:", xgb1_scores.mean())

# Fit the model on the training data
xgb1_classifier.fit(X_train, y_train)

# Evaluate the model on the testing data
test_accuracy = xgb1_classifier.score(X_test, y_test)
print("Testing Accuracy:", test_accuracy)

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import classification_report

# Assuming you have defined your features (X) and target variable (y)

xgb1_classifier = RandomForestClassifier()

# Perform cross-validation predictions
y_pred = cross_val_predict(xgb1_classifier, features, target, cv=5)

# Generate classification report
report = classification_report(target, y_pred)

print(report)

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
from sklearn import metrics

# select the features and target variable
features = ['District', 'Neighborhood','CrimeCode', 'Month', 'Year']
target = ['Description']

# split the data into training and testing sets
train, test = train_test_split(df_new_copy, test_size=0.3, random_state=42)

# Create Naive Bayes classifer object
gnb = GaussianNB()
gnb.fit(train[features], train[target].values.ravel())

# Predict the labels for testing set
predictions = gnb.predict(test[features])

# Calculate the accuracy of the classifier on the testing set
accuracy = accuracy_score(test[target], predictions)
print("Accuracy:", accuracy)

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score

# Select the features and target variable
features = df_new_copy[['District', 'Neighborhood', 'CrimeCode', 'Month', 'Year']]
target = df_new_copy['Description']

# Create the XGBoost classifier object
gnb_classifier = GaussianNB()

# Perform cross-validation
gnb_scores = cross_val_score(gnb_classifier, features, target, cv=5)  # Specify the number of folds (cv=5)

# Print the cross-validation scores
print("Cross-Validation Scores:", gnb_scores)
print("Mean Accuracy:", gnb_scores.mean())

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import classification_report

# Assuming you have defined your features (X) and target variable (y)

gnb = GaussianNB()

# Perform cross-validation predictions
gnb_y_pred = cross_val_predict(gnb, features, target, cv=5)

# Generate classification report
gnb_report = classification_report(target, gnb_y_pred)

print(gnb_report)

plt.plot(range(1, 6), gnb_scores, marker='o')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.title('Cross-Validation Results for Naive Bayes')
plt.show()

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score, train_test_split

# Select the features and target variable
features = df_new_copy[['District', 'Neighborhood', 'CrimeCode', 'Month', 'Year']]
target = df_new_copy['Description']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)

# Create the Gaussian Naive Bayes classifier object
gnb1_classifier = GaussianNB()

# Perform cross-validation on the training data
num_folds = 5
gnb1_scores = cross_val_score(gnb1_classifier, X_train, y_train, cv=num_folds)

# Print the cross-validation scores
print("Cross-Validation Scores:", gnb1_scores)
print("Mean Accuracy:", gnb1_scores.mean())

# Fit the model on the training data
gnb1_classifier.fit(X_train, y_train)

# Evaluate the model on the testing data
test_accuracy = gnb1_classifier.score(X_test, y_test)
print("Testing Accuracy:", test_accuracy)

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import classification_report

# Assuming you have defined your features (X) and target variable (y)

gnb1_classifier = GaussianNB()

# Perform cross-validation predictions
gnb1_y_pred = cross_val_predict(gnb1_classifier, features, target, cv=5)

# Generate classification report
gnb1_report = classification_report(target, gnb1_y_pred)

print(gnb_report)

"""Neural Network"""

import tensorflow as tf
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Split the dataset into features and target
features = ['District', 'Neighborhood','CrimeCode', 'Month', 'Year']
target = ['Description']
X = df_new_copy[features]
y = df_new_copy[target]

# Label encode the target variable
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y.values.ravel())

# One-hot encode the target variable
one_hot_encoder = OneHotEncoder(sparse=False)
y = one_hot_encoder.fit_transform(y.reshape(-1, 1))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define the neural network model
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(14, activation='softmax'))
model.summary()

# Initialize the model
model = tf.keras.models.Sequential()

# Add the input layer and the first hidden layer
model.add(tf.keras.layers.Dense(units=6, activation='relu', input_dim=5))

# Add the output layer
model.add(tf.keras.layers.Dense(units=4, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model on the training data
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)

# Evaluate the model on the testing data
loss, accuracy = model.evaluate(X_test, y_test, verbose=1)
print('Test loss:', loss)
print('Test accuracy:', accuracy)

from sklearn.model_selection import KFold
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Define the neural network model
def create_model():
    model = Sequential()
    model.add(Dense(64, activation='relu', input_dim=X.shape[1]))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Split the dataset into features and target
features = ['District', 'Neighborhood', 'CrimeCode', 'Month', 'Year']
target = ['Description']
X = df_new_copy[features]
y = df_new_copy[target]

# Label encode the target variable
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y.values.ravel())

# Perform K-fold cross-validation
k = 5  # Number of folds
kf = KFold(n_splits=k, shuffle=True)

accuracies = []  # List to store cross-validation accuracies

for train_index, val_index in kf.split(X):
    # Split data into training and validation sets
    X_train, X_val = X.iloc[train_index], X.iloc[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # Create and train the model
    model = create_model()
    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)

    # Evaluate the model on the validation set
    _, accuracy = model.evaluate(X_val, y_val, verbose=0)
    accuracies.append(accuracy)

# Calculate and print the mean accuracy across all folds
mean_accuracy = np.mean(accuracies)
print("Mean Accuracy:", mean_accuracy)

from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Define the neural network model
def create_model():
    model = Sequential()
    model.add(Dense(64, activation='relu', input_dim=X.shape[1]))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Split the dataset into features and target
features = ['District', 'Neighborhood', 'CrimeCode', 'Month', 'Year']
target = ['Description']
X = df_new_copy[features]
y = df_new_copy[target]

# Label encode the target variable
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y.values.ravel())

# Perform K-fold cross-validation
k = 5  # Number of folds
kf = KFold(n_splits=k, shuffle=True)

accuracies = []  # List to store cross-validation accuracies
predictions = []  # List to store predicted labels

for train_index, val_index in kf.split(X):
    # Split data into training and validation sets
    X_train, X_val = X.iloc[train_index], X.iloc[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # Create and train the model
    model = create_model()
    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)

    # Evaluate the model on the validation set
    _, accuracy = model.evaluate(X_val, y_val, verbose=0)
    accuracies.append(accuracy)

    # Predict probabilities for the validation set
    y_prob = model.predict(X_val)
    y_pred = np.argmax(y_prob, axis=1)
    predictions.append(y_pred)

# Calculate and print the mean accuracy across all folds
mean_accuracy = np.mean(accuracies)
print("Mean Accuracy:", mean_accuracy)

# Concatenate the predicted labels from all folds
y_pred_all = np.concatenate(predictions)

# Decode the predicted labels
y_pred_all_decoded = label_encoder.inverse_transform(y_pred_all)

# Decode the true labels
y_true_decoded = label_encoder.inverse_transform(y)

# Generate and print the classification report
report = classification_report(y_true_decoded, y_pred_all_decoded)
print("Classification Report:\n", report)

from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Define the neural network model
def create_model():
    model = Sequential()
    model.add(Dense(64, activation='relu', input_dim=X.shape[1]))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Split the dataset into features and target
features = ['District', 'Neighborhood', 'CrimeCode', 'Month', 'Year']
target = ['Description']
X = df_new_copy[features]
y = df_new_copy[target]

# Label encode the target variable
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y.values.ravel())

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create the model
model = create_model()

# Fit the model on the training data
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)

# Evaluate the model on the testing data
_, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print("Testing Accuracy:", test_accuracy)

# Perform K-fold cross-validation
k = 5  # Number of folds
kf = KFold(n_splits=k, shuffle=True)

accuracies = []  # List to store cross-validation accuracies
predictions = []  # List to store predicted labels

for train_index, val_index in kf.split(X_train):
    # Split data into training and validation sets
    X_train_fold, X_val = X_train.iloc[train_index], X_train.iloc[val_index]
    y_train_fold, y_val = y_train[train_index], y_train[val_index]

    # Create and train the model
    model = create_model()
    model.fit(X_train_fold, y_train_fold, epochs=50, batch_size=32, verbose=0)

    # Evaluate the model on the validation set
    _, accuracy = model.evaluate(X_val, y_val, verbose=0)
    accuracies.append(accuracy)

    # Predict probabilities for the validation set
    y_prob = model.predict(X_val)
    y_pred = np.argmax(y_prob, axis=1)
    predictions.append(y_pred)

# Calculate and print the mean accuracy across all folds
mean_accuracy = np.mean(accuracies)
print("Cross-Validation Accuracy:", mean_accuracy)

# Concatenate the predicted labels from all folds
y_pred_all = np.concatenate(predictions)

# Decode the predicted labels
y_pred_all_decoded = label_encoder.inverse_transform(y_pred_all)

# Decode the true labels
y_true_decoded = label_encoder.inverse_transform(y_train)

# Generate and print the classification report
report = classification_report(y_true_decoded, y_pred_all_decoded)
print("Classification Report:\n", report)